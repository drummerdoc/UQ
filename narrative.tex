\documentclass[11pt]{article}
% RFP specifically says to use 11 point type and 1 inch margins
\usepackage{graphicx}
\usepackage{epsf,color}
\textwidth=6.5in\oddsidemargin=0in \evensidemargin=0in \topmargin
0pt \advance \topmargin by -\headheight \advance \topmargin by
-\headsep \textheight 9.0in

%\textwidth=6.5in\oddsidemargin=0in \evensidemargin=0in \topmargin
%0pt \advance \topmargin by -\headheight \advance \topmargin by
%-\headsep \textheight 8.9in

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage[compact]{titlesec}
\usepackage{url}

%\usepackage[plain]{fullpage}
\usepackage{amsfonts}
%\usepackage{lastpage}
%\usepackage{fancyhdr}

\usepackage[version=3]{mhchem} 
% you can use this command to skip chunks of your document
% just put the command around the chunk like this
% \comment{ ...the chunk... }
\newcommand{\comment}[1]{}

%\newcommand{\MarginPar}[1]{\hspace{1sp}\marginpar{\tiny\sffamily\raggedright\hspace{1sp}#1}}
\setlength{\marginparwidth}{0.75in}
\newcommand{\MarginPar}[1]{\marginpar{%
\vskip-\baselineskip %raise the marginpar a bit
\raggedright\tiny\sffamily
\hrule\smallskip{\color{red}#1}\par\smallskip\hrule}}

%\renewcommand{\baselinestretch}{1.05} % = 1.0 Single space; = 2.0 Double
\renewcommand{\baselinestretch}{1.0} % = 1.0 Single space; = 2.0 Double

%\renewcommand{\refname}{Literature Cited}
%------------------------

%\pagestyle{empty}  % No page numbers
%\textfloatsep 0mm
%\abovecaptionskip 1mm

\begin{document}

%\pagestyle{plain}
%\pagenumbering{roman}
\begin{center}
{\large{\textbf{A Hierarchical Approach to Uncertainty Quantification}}}
\end{center}

\subsection*{Background / Introduction}

Many important DOE applications rely on simulation to predict the behavior of complex physical systems.
However, the fidelity of these simulations depends on uncertain parameters describing the underlying physical system
obtained from a collection of
complex and noisy experiments whose reliability is hard to determine. 
Our ability to effectively use extreme scale computing will depend critically on our ability to reduce the
uncertainty in these data and assess the impact of that uncertainty on predictive capability.
Here, we will focus on combustion modeling, battery simulation and design of high-efficiency photovoltaic
devices as motivating examples.
\MarginPar{a bit more detail here?}
However, the methodology will be broadly applicable to a
range of problems in chemical and materials science, systems biology, subsurface flow and climate to name a
few.

An important aspect of the class of problems we plan to consider is that 
the system is not probed using a single experiment.  Instead,
there is typically a hierarchy
of experiments of increasing complexity that provide information about the underlying processes
that lead up to the desired target application.
As experimental complexity increases the fraction of the state that can be sampled
by measurement is reduced and the cost of simulations increases.
Rather than attempting to estimate parameters directly from a single complex experiment, we 
will obtain parameter estimates across the entire hierarchy of experiments.
Thus, we need an approach that allows us to pass
information through the hierarchy in a way that effectively uses data from all levels
to improve overall predictive properties.
Furthermore, as complexity increases,
the combination of rich physics and relative data sparsity suggests that we will not be
able to exactly match the experimental dynamics computationally.
We must develop estimation methods that are robust to model errors as well as noisy observations
using metrics based on identification of characteristic features that are more general than
traditional quantities of interest.

The goal of this project is to develop a
mathematical framework for this class of problems that
will utilize data from a hierarchy of experiments of increasing complexity to reduce
uncertainty of simulations, estimate the impact of the improved characteristic on predictive capability
and identify significant remaining sources of uncertainty.
%intertwine parameter estimation and simulation into an integrated activity.
Our approach is based on novel Monte Carlo (MC) sampling approaches within a Bayesian uncertainty quantification (UQ) framework. 
The Bayesian approach combines modeling and sampling in a way that provides full insight into the propagation of 
uncertainty.
The result is a complete characterization of the posterior probability density function (pdf) which contains all information about remaining uncertainties that is implied by the data -- which aspects are tightly bounded and which are less precise.

From a practical perspective, an implementation of the Bayesian UQ approach via MC sampling
is attractive because it respects the strong nonlinearities in the target applications
and avoids the need for simplifying assumptions/approximations required by some current UQ techniques.
Moreover, some MC sampling methods are well suited for massively parallel computer
architectures because the computationally expensive calculations,
e.g. forward or adjoint model runs, can be executed independently and
the communication between cores for each independent run is small.
Using MC sampling as the computational backbone will lead to a numerically sound
implementation of a rigorous UQ theory that is well suited for extreme scale machines, making UQ possible for realistic applications.

MC sampling techniques approximate the posterior distributions that are the basis of Bayesian UQ by a set of samples. However, computing these samples can be very hard. A key issue is that many current MC techniques are not robustly effective for complex, high-dimensional problems. There is a weak analogy to the problem of solving large systems of linear or nonlinear equations.
For equation solving there are generic methods such as Gaussian elimination, Gauss Seidel iteration,
Newton's method, etc.
But these generic methods are unable to solve extreme scale problems that arise from simulating
complex physics in 3D.
These require specialized multi-scale algorithms, such as multigrid, that take into account the
physical nature of slowly relaxing modes.
What this analogy fails to capture is an extra difficulty in sampling: these problems do not come with 
something like {\em residuals} that can be evaluated to confirm convergence of the algorithms.
There is no way to test a-posteriori that a sampler has converged to the correct distribution.
The correctness must be built in to the sampler.

Standard Markov chain Monte Carlo (MCMC) samplers can become very slow when the distribution being
sampled is ill conditioned.
Ill conditioning in problems of moderate dimension can come from approximate degeneracy: Very
different combinations of parameters fit the data nearly as well.
Several approaches are emerging to address this, including multi-stage adaptive samplers and
affine invariant ensemble samplers.
High dimensional problems that come from discretizing field theories or stochastic PDEs
can be ill conditioned because of the range of modes at different length scales.
Hamiltonian samplers have proven better than direct local Metropolis or heat bath methods \cite{Hanson2001}, but
they still can be slow.
More sophisticated truly multi-scale samplers include Swendsen Wang \cite{SwendsonWang1987}
and multigrid methods \cite{Goodman1989}.
Unfortunately, none of these methods at present is nearly as general, say, as multigrid for
solving discretizations of PDEs.
Some promising developments include ``chainless'' approximate methods \cite{Chorin:2008}, and
{\em parallel marginalization} \cite{Weare2007}.
We will look for multiscale sampling methods that are effective for problems that come from PDE
with noise under partial observation.

We can consider different MC sampling techniques (the Bayesian approach is conceptually independent of its implementation) and we plan to investigate which sampling method is applicable at the various levels of the hierarchy. In particular, we will consider particle filters (importance sampling) as an alternative to MCMC. Standard particle filters first obtain samples of the prior and then evaluate their probability with respect to the posterior by attaching weights to each sample based on its distance to the data \cite{Doucet2001,GordonSIR}.
The weighted particles form an empirical estimate of the posterior.
Difficulties arise if the prior and posterior approach are mutually singular, i.e. if high-probability events with respect to the prior have low probability with respect to the posterior.
However, this is the interesting case, which corresponds to the situation where one can learn from the data, rather than simply confirming the prior.
Under these conditions many of the samples have a low weight so that the empirical estimate they constitute is poor (it can be a single and often unlikely point).
Although this can happen when estimating a single parameter, it was shown rigorously for linear problems that the number of particles required scales catastrophically with the number of parameters being estimated \cite{Bickel,BickelBootstrap,Bickel2,Snyder,Weare2012,Weare2009}, making this approach infeasible for our target applications.
A number of methods have been invented to ameliorate this problem, most of which amount to finding a proposal density that  generates samples that are more compatible with the data \cite{Doucet,OptimalImportanceFunction,liuchen1995,Brad}.
For example, a nudging technique was proposed in meteorology, however the method is not theoretically sound, requires a lot of ad hoc tuning  and is not easily applicable to parameter estimation \cite{vanLeeuwen}.
A more general approach constructs a map that transforms the prior measure into the posterior measure
%, i.e. to construct a map that transforms prior samples into posterior samples 
\cite{Moselhy2013}.  If such a map could be constructed, efficient posterior samples are easy to generate.
However, the construction of the map is theoretically and computationally expensive and, therefore, relies on a number of approximations (e.g. polynomial representations of the maps) that are not efficient.
We expect that we will need to compute a map at each level of the hierarchy and, thus, expect that this approach is computationally too expensive for our target applications. Moreover, the errors of the approximations involved in finding the map are, at this time, not well understood. We plan to base our particle filtering methods on the implicit sampling methodology invented at LBNL \cite{chorintupnas,chorin2010,Morzfeld2011,Morzfeld2012,Atkins2013}, which avoids some of the pitfalls of standard sampling by concentrating the computational power on localizing the regions of high probability in the parameter space (see below for details of this method).


\MarginPar{George: Rewrote ROM\\ Matti: I did another round of re-writing and hope I didn't make it worse and JBB merged them}
The posterior samples obtained from sampling (either MCMC or implicit sampling) can be used to construct reduced order models (ROMs), by which we mean inexpensive (to evaluate) surrogates for expensive computer simulations that preserve the input-output relations in the numerical models.
Here, we will use the term ROM in an encompassing sense which can refer to model reduction approaches, response surface approaches and machine learning techniques. For example, in model reduction approaches, e.g. the proper orthogonal decomposition method~\cite{Cardoso:2009jn, Lieberman:2010dw, Willcox:2002uk}, reduced basis method~\cite{Prudhomme:2002ug,Quarteroni:2011jm} and trajectory piecewise linearization procedure~\cite{Cardoso:2010, Rewienski:2003tr}, one directly approximates the PDEs using a parametrically-induced approximation space.
However, these techniques are difficult to apply to complex, multiscale models.

Response surface and machine learning techniques
such as the Gaussian process regression (GPR)~\cite{Rasmussen:2006vz}, artificial neural network (ANN)~\cite{Cybenko:1992vo} and polynomial chaos expansion (PCE)~\cite{Oladyshkin:2012ur} are simpler to use, but possess their own severe limitations. 
PCE, in particular has been applied successfully to a limited class of flow problems (e.g. \cite{GhanemDham1998, LeMaitre2001}).  In PCE methods, the probability space of the parameters is decomposed into a finite set of univariate polynomials with local or global support, and coupled together through tensor products.  These bases functions are propagated through the model, either ``intrusively'' or ``unintrusively''.  Intrusive schemes utilize, e.g., a Galerkin projection to reformulate the governing equations into a system governing the PC mode strengths directly; unintrusive methods numerically evaluate the modes through deterministic or random sampling of the original model/code.  However, strong nonlinearity and complex unsteady dynamics present significant challenges for this approach \cite{Mathelin2004}. 
The difficulties arise primarily from the high-order expansions necessary to capture the modes with sufficient accuracy.
In dynamic simulations, PCE usually requires a rapidly increasing expansion order to maintain a constant accuracy in time. 
Specialized applications exist for oscillatory systems (e.g. vortex-shedding \cite{LucorKarniadakis2004,Xiu2002}), including phase-locked wavelet-based approaches \cite{WitteveenBijl2008}, but the general time-dependent problems of interest here remain computationally intractable for PCE. 
GPR and ANN methods offer no relief, since much of the functional relationships between the outputs and parameters of interest encoded within the numerical models are lost, and must be indirectly captured through a prohibitively large set of training data.  

%There are several areas in which ROMs can play a role in a MC algorithm. 
A research question we will address is how to combine ROMs with MC algorithms as we move through the hierarchy of experiments.
For example, coarse resolution (thus computationally cheap) models have been used to generate  samples
during the importance sampling stage~\cite{Higdon:2002vx,Christen:2005wp,Efendiev:2007uw,Bal:2013tp} or
coupled with fine resolution models through the metropolis coupled chain~\cite{Higdon:2002vx}.
To further improve the match between the coarse and fine posterior distributions, Bal et. al.~\cite{Bal:2013tp} used
a zero-mean Gaussian model to approximate the discretization error~\cite{Kaipio:2007ux}.
However, since our simpler models may involve significant simplifications of the physics or reduction of spatial dimensions,
ROMs may be needed to more accurately model the structural variations of the difference between the models. 
Gaussian process regression are a potentially attractive form of ROM to capturing differences in resolution of models.
ROMs can also be used directly as an efficient surrogate in forward UQ~\cite{Challenor:2012uv, Ratto:2012tf} for example in predicting catastrophic events and obtaining global sensitivity analyses indices that converges slowly with number of samples.   In this proposal, we will look into how ROMs can further improve the efficiency of the importance sampling approach that we will use.  In addition, we will determine how ROM can be used within our hierarchical framework to efficiently obtain an accurate predictions to pin the forward UQ.


%Response surface and machine learning techniques such as the Gaussian process regression (GPR)~\cite{Rasmussen:2006vz}, artificial neural network~\cite{Cybenko:1992vo} and polynomial chaos expansion~\cite{Oladyshkin:2012ur} are simpler to use but the functional relations between the outputs and parameters of interest encoded within the numerical models are lost, and must be indirectly captured through a large amount of training data.
%A research question we will address is how to combine ROMs with MC algorithms as we move through the hierarchy of experiments.
%For example, coarse resolution (thus computationally cheap) models can be used to generate samples during the importance sampling stage~\cite{Higdon:2002vx,Christen:2005wp,Efendiev:2007uw,Bal:2013tp} or coupled with fine resolution models through the metropolis coupled chain~\cite{Higdon:2002vx}.
%To improve the match between the coarse and fine posterior distributions, Bal et. al.~\cite{Bal:2013tp} used a zero-mean Gaussian model to approximate the discretization error~\cite{Kaipio:2007ux}.
%However, we expect that this simple approach will not be sufficient for our target applications since the simpler models we expect to encounter involve significant simplifications of the physics and / or reduction of spatial dimensions. We plan to use ROMs to model the structural variations of the difference between the models at the various levels of the hierarchy.
%In addition, we will address how to use ROMs within our hierarchical framework to obtain accurate and efficient forward UQ techniques ~\cite{Challenor:2012uv, Ratto:2012tf}, for example for predicting catastrophic events and obtaining global sensitivity analysis indices. 
%=======
%Both MCMC and importance sampling approximate the posterior distribution of the targeted parameters by a (large) number of samples. With marginal additional costs, these samples can further be fruitfully used to create reduced order models (ROMs) that can be used to improve efficiency at the various stages of the MC algorithms.  ROMs are inexpensive surrogates for expensive computer simulations that aim to preserve the input-output relations in the numerical models. 
%In this proposal, the term is used in an encompassing sense to refer to model reduction approaches, response surface approaches and machine learning techniques.
%Model reduction approaches, such as the proper orthogonal decomposition method~\cite{Cardoso:2009jn, Lieberman:2010dw, Willcox:2002uk}, reduced basis method~\cite{Prudhomme:2002ug,Quarteroni:2011jm} and trajectory piecewise linearization procedure~\cite{Cardoso:2010, Rewienski:2003tr} directly approximate the PDEs using a parametrically-induced approximation space. 
%However, these techniques can be difficult to apply to complex, multiscale models directly.  

%Response surface and machine learning techniques
%such as the Gaussian process regression (GPR)~\cite{Rasmussen:2006vz}, artificial neural network (ANN)~\cite{Cybenko:1992vo} and polynomial chaos expansion (PCE)~\cite{Oladyshkin:2012ur} are simpler to use, but possess their own severe limitations. 
%PCE, in particular has been applied successfully to a limited class of flow problems (e.g. \cite{GhanemDham1998, LeMaitre2001}).  In PCE methods, the probability space of the parameters is decomposed into a finite set of univariate polynomials with local or global support, and coupled together through tensor products.  These bases functions are propagated through the model, either ``intrusively'' or ``unintrusively''.  Intrusive schemes utilize, e.g., a Galerkin projection to reformulate the governing equations into a system governing the PC mode strengths directly; unintrusive methods numerically evaluate the modes through deterministic or random sampling of the original model/code.  However, strong nonlinearity and complex unsteady dynamics present significant challenges for this approach \cite{Mathelin2004}. 
%The difficulties arise primarily from the high-order expansions necessary to capture the modes with sufficient accuracy.
%In dynamic simulations, PCE usually requires a rapidly increasing expansion order to maintain a constant accuracy in time. 
%Specialized applications exist for oscillatory systems (e.g. vortex-shedding \cite{LucorKarniadakis2004,Xiu2002}), including phase-locked wavelet-based approaches \cite{WitteveenBijl2008}, but the general time-dependent problems of interest here remain computationally intractable for PCE. 
%GPR and ANN methods offer no relief, since much of the functional relationships between the outputs and parameters of interest encoded within the numerical models are lost, and must be indirectly captured through a prohibitively large set of training data.  

%There are several areas in which ROMs can play a role in a MC algorithm. 
%For example, coarse resolution (thus computationally cheap) models have been used to generate  samples  during the importance sampling stage~\cite{Higdon:2002vx,Christen:2005wp,Efendiev:2007uw,Bal:2013tp} or coupled with fine resolution models through the metropolis coupled chain~\cite{Higdon:2002vx}.
%To further improve the match between the coarse and fine posterior distributions, Bal et. al.~\cite{Bal:2013tp} used a zero-mean Gaussian model to approximate the discretization error~\cite{Kaipio:2007ux}.
%However, since our simpler models may involve significant simplifications of the physics or reduction of spatial dimensions,
%ROMs may be needed to more accurately model the structural variations of the difference between the models. 
%ROMs can also be used directly as an efficient surrogate in forward UQ~\cite{Challenor:2012uv, Ratto:2012tf} for example in predicting catastrophic events and obtaining global sensitivity analyses indices that converges slowly with number of samples.   In this proposal, we will look into how ROMs can further improve the efficiency of the importance sampling approach that we will use.  In addition, we will determine how ROM can be used within our hierarchical framework to efficiently obtain an accurate predictions to pin the forward UQ.

%We intend to examine how we can use reduced order modeling techniques to model this error accurately.
%In particular, Gaussian process regression~\cite{Rasmussen:2006vz} is expected to fit within the statistical framework that we will describe.
%We note that reduced order models can also be directly used as the simpler model~\cite{BuiThanh:2012tx}.
%For implicit sampling, reduced order models have the potential of reducing the cost of the required optimizations, either through the use of the simpler models in the optimizations or  via more direct optimization algorithms that utilize reduced order models~\cite{Regis:2007,Wild:2011uh}. 


%We will also address forward UQ, including UQ of predictions made with the posterior computational models as well as sensitivity analysis used in decision making and experimental design. While multilevel Monte Carlo can improve the convergence rate for a series of convergent models~\cite{Giles:2008gc},  a theoretical framework for a heterogeneous set of models is lacking. Alternatively one can use surrogates for fine-scale models in the (posterior) forward  analyses~\cite{Challenor:2012uv, Ratto:2012tf} and we will investigate how these reduced order models can be accurately and efficiently used in a hierarchical framework. 

% Although efficiency Monte Carlo of sampling techniques has been significantly improved in recent years, many problems are still computationally intractable, even with the computational gain achieved through high-performance computing efforts (Wang et al., 2011). One feasible solution is to use reduced-order models (ROMs), or emulators, as efficient surrogates for fine-scale models in these analyses (Challenor, 2012; Ratto et al., 2012).  

\subsection*{Proposed Research}

To frame the discussion of the proposed research we need to first provide some specific detail
about the target applications.

\subsubsection*{Combustion}
Simulating the combustion of a given fuel relies on parameters
describing chemistry, transport and thermodynamics.
Here we will assume that the multicomponent reacting Navier Stokes equations with Arrhenius
kinetics and a molecular transport model provide a good approximation to the dynamics.
The target question we would like to answer is how confident are we in the computed statistical
properties of a turbulent flame simulation and to what extent are the simulations consistent
with experimental data.
However, a direct attack on this problem is infeasible.
The computational complexity is sufficiently high and the available data is sufficiently
sparse that very little can be determined by directly pursuing the final target.
A way to get around this is to note that one has not simply a single turbulent flame
experiment but a wide range of different types of experiments for the same physical
system of increasing complexity.  
Hierarchical data sources (experiments): 0d ignition, 1d laminar flames, shock-tube experiments,
a variety of 2D laminar flames, and 3D turbulent flames.
Each experiment can also have a number of distinct sets of data that focus on different
aspects of the flame.
Some experiments measure velocities using particle image velocimetry. Other experiments
measure composition at a given location using Raman spectroscopy. Others use planar laser induced
fluorescence to indirectly measure a particular species in a cross section of the flow.
One could potentially augment this set of experiments and data with other types of non-reacting flow experiments and corresponding data that focus on transport properties. Since each of these experiments reveal information about the system (some are used by kineticists already), the idea would be to use experimental data across the entire hierarchy of experiments to reduce uncertainty in parameters and estimate bounds on how those uncertainties impact predictive capability.

Our UQ approach needs to reflect the notion that as we move up the hierarchy, we are
getting closer to realistic applications but the simulations become
increasingly costly and the data become increasingly sparse.
Moreover, the approach needs to respect the structure of the problem.
A reasonable cartoon for a 
turbulent flame experiment is that it corresponds to a stationary chaotic dynamical system.
What the experimentalist is capturing, at some level, is
snapshots of what the attractor looks like.
We cannot hope to have a complete picture of the flame at a given time, because available data is at best limited. For example, boundary conditions are only known statistically or we only have a qualitative characterization of selected features.
However, the data carries some information about the underlying physical system and our goal is to extract this information from the limited data we have access to.

\subsubsection*{Thin-film photovoltaic materials}
Our second target application 
is the formation of point defects in thin-film photovoltaic
materials. 
\MarginPar{JBB:  maybe too long}
Thin film photovoltaic (PV) devices have attracted research attention
as a way to dramatically reduce the cost of solar panels. These
materials compete with traditional crystalline silicon based on
efficiency but use far less
material \cite{JiangY13}. Leading thin film technologies include absorbers based on
cadmium telluride (CdTe), Copper-Indium-Gallium-Selenide/Sulfide (CIGS) and $Cu_2ZnSnS_4$ (CZTS) \cite{JiangY13}. In contrast
to the exotic elements in CdTe and CIGS, CZTS is
comprised of relatively abundant and benign constituents. However,
despite considerable attention CZTS development is still not mature
relative to CdTe technology. The performance is sensitive to the
fabrication process: leading demonstrations for CZTS successful
fabrication processes include depositing the film on a substrate via
solution-processing (11\% conversion efficiency) \cite{Todorov13},
co-evaporation (9.15\% efficiency \cite{Repins12})) and vacuum
process (8.4\% efficiency, \cite{Shin11}). Despite theoretical
efficiencies exceeding 30\%, developing optimum processing conditions
that realize the potential remain elusive.
The challenge here is to 
refine estimates of the model parameters
sufficiently to improve predictions of the process
from of measurements of bulk
properties that result from uncertain and time-varying process
conditions.

Unlike crystalline silicon that is explicitly doped with phosphorous
or boron to create n-type or p-type semiconductors, CZTS is
`self-doped' through point defects in the kesterite crystal. Creation
of these defects (vacancies, antisite and interstitial), particularly
the $Cu_{Zn}$ antisite defect for p-type doping in CZTS, results in an
increase in carrier concentration and semiconductor behavior
\cite{JiangY13}. The overall defect formation/destruction, movement
through the crystal structure and along grain boundaries can be
described by a set of coupled stochastic partial differential
equations that treat the point defect concentrations as continuum
quantities:
\begin{equation}
  \label{eq:pdpde}
  \frac{\partial \phi_j}{\partial t}  = S_j + \nabla \cdot  D_j \nabla \phi_j  + T_j,
\end{equation}
where $\phi_j$ represents the number density of $j^\mathrm{th}$ defect or election/hole (carrier) concentration. \MarginPar{(matti) what is $D_j$ and$T_j$?}

The source term $S_j$ is local rate of change due to pseudo-reactions
that govern the creation/destruction and interaction of point
defects. For example, a simple model of a metal-X system  can
be described by a simplistic system of four reactions. 

\begin{eqnarray}
\ce{ \tfrac{1}{2} X_2 -> X_x + V_m^{''} + 2 h^+ } \\
\ce{ NULL -> V_m^{''}+ V_x^{\cdot \cdot} } \\
\ce{ NULL -> e^ '+ h^{\cdot} } \\
\ce{ X_x -> \tfrac{1}{2} X2 + V_x^{\cdot \cdot} + 2 e^- }.
\label{eq:pdrxns}
\end{eqnarray}
In the above set, $\ce{X_2}$ is a non-metal gas phase that is interacting
with the crystal at the interface. The first reaction represents the
addition of an $\ce{X}$ atom to the lattice producing cation vacancies \MarginPar{(matti) Do the X's here need indices? What about the other symbols: $\ce{NULL},  \ce{h^{+}}, \ce{h^{\cdot}}, \ce{e^{\cdot}}, \ce{e^+}$?}
($V_m^{''}$); the last reaction represents an X ion in the lattice
dissociating into an X atom that leaves the lattice and two elections,
leaving behind an anion vacancy ($V_x^{\cdot \cdot}$). The second and
third reactions represent creation and elimination of vacancies
(``Schottky defects'') and creation/elimination of electron/hole pairs
(``electronic defects'') \cite{Tilley}. The equilibrium concentration
of these defects are non-zero because the free energy of the crystal
is lowered in the presence of such defects; in a more complicated
system such as CZTS many more defects are possible. Electronic
structure calculations \cite{Walsh12} provide a wealth of information
indicating the important physical processes and configuration effects,
but are unable to capture the long time-horizon dynamics necessary to
couple the processing conditions to the concentrations. The rates of
the reactions in (\ref{eq:pdrxns}) are given by Arrhenius forms
($\omega_\alpha = k_\alpha e^{-(E_a)_\alpha/(RT)}$) and then assembled
into the source terms $S_j$ based on a linear combination given by the
coefficients for the $j^\mathrm{th}$ species in the
$\alpha^\mathrm{th}$ reaction.

Transport of the point defects within the crystal occurs in two
regimes. Solid state diffusion is frequently approximated using a
Fickian treatment (2nd term on right hand side (RHS) of Equation \ref{eq:pdpde} with defect-dependent mobility $D_j$),
however for point defects this is insufficient to account for their
migration along grain boundaries, where, as suggested earlier, the
transport mechanism changes and becomes delocalized. Higher
probability of the defect moving along the grain boundary can be
captured by a stochastic model more naturally than a deterministic
treatment; the final term on the RHS represents stochastic transport
that is active only in the vicinity of a grain boundary. Kolluri and
Demkowicz \cite{Kolluri12} looked into transport of delocalized
defects at interfacial grain boundaries for CuNb and found that it can
be modelled as a complication on localized defect migration and
successfully treated based on transition state theory with a
temperature dependent pre-factor. \MarginPar{(matti) can you check the last sentence? I am a bit confused here.}

The importance of grain boundaries for transport requires a model for
grain nucleation and growth. CZTS has a complicated phase diagram with
\MarginPar{JBB:  huh?}
a narrow region where the CZTS phase is present. The effects of the
secondary phase presence on device performance is throughly described
in, e.g., \cite{Flammersberger}, but are largely detrimental to device
performance, so the process of grain growth under annealing conditions is an important processing step.  Grain boundaries between phases also alter the driving
force for point defect migration that is notoriously difficult to
capture deterministically; stochastic models have been much more
successful \cite{Koptelov84}. Grain structure has been treated by numerous methods that attempt to capture the driving force for grain growth ( to first order, the reduction in grain energy captured by the curvature of the interface and interactions between the grain) including interface tracking and continuum descriptions. The latter, where grain boundaries are identified as misaligned orientations between adjacent points on a (typically cartesian) has been formulated as a deterministic \cite{FanGC97} or stochastic \cite{Rollett04} problems is conveniently compatible with the continuum representation of point defect kinetics.  In the deterministic treatment by Fan et al., a set of ODEs is written for a finite number $p$ of continuous orientation field variables at each grid point $\eta_1, \eta_2(x), \dots, \eta_p(x)$. The total free energy is then written in terms of the local free energy density $e_0$ and gradient energy coefficients $\kappa_i$:
\MarginPar{JBB:  may need to shorten this.  Can we credibly attack this complexity}
\begin{equation}
  \label{eq:detgrainE}
  E = \int_V e_0\left( \eta_1, \eta_2, \dots, \eta_p \right) + \sum_i^p \frac{\kappa_i}{2} \left( \nabla \eta_i \right)^2 dx.
\end{equation}
Fan et al. use a function form for the free energy density involving phenomenological parameters $\alpha, \beta, \gamma$:

\begin{equation}
  \label{eq:detgraine0}
  e_0 = \sum_i^p \left( - \frac{\alpha}{2} \eta_i^2 + \frac{\beta}{4}\eta_i^4\right) + \gamma\sum_i^p \sum_{j\ne i}^p \eta_i^2\eta_j^2,
\end{equation}
and used the Ginzburg-Landau equations involving kinetic coefficients describing grain boundary mobility $M_i$ to evolve the orientation field variables:
\MarginPar{JBB what is $L_i$}
\begin{equation}
  \label{eq:detgrainGL}
  \frac{d \eta_i}{dt} = -L_i \frac{\delta E}{\delta \eta_i}= -L_i \left( \frac{\partial e_0}{\partial \eta_i} - \kappa_i \nabla^2\eta_i \right)
\end{equation}

In the stochastic formulation used by Rollett, each possible orientation is enumerated a priori into an orientation index. Each gridpoint is assigned an orientation index $S$, with grains corresponding to contiguous regions with constant S. The total system energy is defined as:
\begin{equation}
  \label{eq:mcgrainE}
  E = \sum_j^N \sum_i^n J(S_i, S_j) (1-\delta_{S_iS_j})
\end{equation}
where the inner sum is taken over the $n$ first and second nearest neighbors of the $i^\mathrm{th}$ element and $J(S_i,S_j)$ is the energy of a unit of boundary between elements $i$ and $j$ with orientation indicies $S_i$ and $S_j$. The transition probability for a reorientation with energy difference $\Delta E$ and mobility $M$ beween indicies $S_i$ and $S_j$ is:
\begin{equation}
  \label{eq:mcgrainP}
  P( S_i, S_j, \Delta E, T) = \left\{ \begin{aligned}
      & \frac{J(S_i,S_j)}{J_\mathrm{max}} \frac{ M(S_i, S_j) }{M_\mathrm{max}}  \quad \Delta E \le 0 \\
      & \frac{J(S_i,S_j)}{J_\mathrm{max}} \frac{ M(S_i, S_j) }{M_\mathrm{max}} \exp \left( \frac{-\Delta E}{TJ(S_i,S_j)}\right)  \quad \Delta E > 0 \\
      \end{aligned}
      \right. 
\end{equation}
In the above, the energy between orientation indicies $J$ and mobility $M$ are experimentally derived parameters for each orientation pair that reflect, preferential growth along a particular misalignment direction. The state of the crystal is updated sequentially by choosing random locations and new orientations that are accepted based on the given probability. 


%\remrg{The stochastic formulation appears to be calling out for MCMC. This aspect of the thin film photovoltaic application readily generalizes and is applicable to a broader range of materials problems, where the objective is to predict grain size after an annealing process to design materials with specific mechanical as well as electrical properties.}


  Solution of the combined system (point defect and carrier
  concentrations augmented by grain growth models) yields the $e$ and
  $h$ carrier concentrations that are used to assess device
  performance. Point defects are generally not directly measurable;
  instead, they are inferred from measurements of carrier
  concentration (via resistance, conductivity). The temperature
  sensitivity of such measurements can be used in combination with
  either a selection of conditions where a particular type of defect
  governs the response (not general) or in combination with the
  solution of the full system as outlined above. A desirable workflow
  for UQ in this application is to use the experiments available to narrow
  the uncertainty in the kinetic and transport parameters as far as
  possible, and then search for processing pathway that provides
  optimum result given remaining uncertainty in parameters.

%  Overall, this application has several aspects that differentiate it
%  from the others insofar as it has an inherently stochastic component
%  (in the transport at the grain boundaries and also, in the more
%  sophisticated models, in the reactions describing the surface
%  kinetics), it exhibits very large uncertainty in relatively few
%  parameters (the bounds on the kinetic constants are very wide) and,
%  as it is somewhat less well studied than the combustion system,
%  there is a significant opportunity for model insufficiency. The
%  final feature provides an opportunity to test the robustness of
%  mechanisms to detect when a model is inconsistent with data.

%%%%%%%%%%% batteries
\subsubsection*{Energy storage}
Our third application area concerns lithium-ion batteries.
A recent DOE Basic Energy Sciences (BES) workshop report states: 
``Revolutionary breakthroughs in Electrical Energy Storage have been singled out as perhaps the most crucial need
for this nation's secure energy future''  (\cite{ees_rpt}, p. xii).
The promise of electric-drive vehicles (EDVs) can only be met if batteries having both high energy density and increased safety 
and reliability are developed.  Current solutions, including exploding laptops and expensive electric vehicles that burst into 
flame and/or can only travel 100 miles before requiring a full night to recharge, are inadequate for widespread adoption.
Lithium-ion batteries (LIBs) present a truly multiscale system, with important macroscopic behavior
(e.g., catastrophic failure such as fire) determined by intricate microscale
interactions (e.g., as a result of dendritic growth on lithium particles in
one location deep inside the battery pack). 
%It is extremely difficult, where
%possible at all, to examine the mechanisms involved---especially, to
%elucidate the microscopic origin of macroscopic phenomenon---experimentally.
The highly structured, heterogeneous nature of battery systems implies we must model a coupled multiphysics system, rather
than one system presumed to govern the full device.   
In theory one can enumerate at least 6 levels of detail that are tied together in some way: 
atomistic (quantum), atomistic (MD), particles, electrode, cell, pack.   Some
\MarginPar{JBB:  suggestions:  here we start with the smallest scale being a paricle.  Don't give 
schroedinger or MC}
representative equations include:

%A representative sample of the types equations encountered at the various scales as is follows:\\
\begin{itemize}\itemsep -0.0em
\item
atomistic, quantum:  (Schr\"{o}dinger) $ H \Psi = \epsilon \Psi$, where $H$ is the Hamiltonian operator, and 
$\Psi$, $\epsilon$ are the wave functions and eigenvalues.
\item
atomistic, molecular dynamics: (Newton's law) $\frac{1}{m}\frac{d^2x}{dt^2} = -\frac{dU}{dt}$, where
$x$, $m$ are atomic positions and masses, and $U$ is the potential energy.
\item
particle (solid) Li concentration $c_s$ (diffusion): $\frac{\partial c_s}{dt} = \nabla \cdot (D_s \nabla c_s)$, where
$D_s$ is the diffusivity of Li in Li-hosting particles.
\item
particle-electrolyte interfacial kinetics, e.g., Butler-Volmer: $I = \exp(\alpha_a \eta(I)) - \exp(\alpha_c \eta(I))$.  The overpotential $\eta(I)$ is a function of Li current $I$ across the interface, and $\alpha_a$, $\alpha_c$
are anode and cathode charge transfer coefficients.  This equation can be derived under certain assumptions from more general chemical kinetic equations.
\item
electrolyte Li concentration $c_e$ (convection-reaction-diffusion): $\frac{\partial c_e}{dt} = \nabla \cdot (D_e \nabla c_e) + \nabla (v c_e) + R_{c_e}$, where $D_e$ is Li diffusivity in electrolyte, $v$ is the 
convective velocity (a function of electric potential $\phi$), and $R_{c_e}$ is the reaction rate (solution of, e.g., Butler
Volmer). 
\item
electrolyte electric potential $\phi$ (reaction-diffusion): $\frac{\partial \phi}{dt} = \nabla \cdot (D_{\phi} \nabla \phi)  + R_{\phi}$, where $D_{\phi}$ is charge diffusivity in electrolyte, $R_{\phi}$ is a volume averaged current
density entering electrolyte.
\item
electrode stress field, $\sigma$, e.g., Hooke's law for chemically active solid: $\sigma_{ij} = C_{ijkl}^{Li} [\epsilon_{kl} - \beta_{kl}(c_e - c_{e_0})] $, where $c_{e_0}$ is equilibrium concentration, and $C$, $\epsilon$, $\beta$ are stress, strain, and Vegard coefficients.
\item
electrode equilibrium stress, $\sigma$ (constitutive equation): $\nabla \cdot \sigma = 0$ 
\item
cell/pack electric potential $\Phi$ (reaction-diffusion): $\frac{\partial \Phi}{dt} = \nabla \cdot (D_{\Phi} \nabla \Phi) + R_{\Phi}$,
where $D_{\Phi}$ is charge diffusivity in cell/pack, $R_{\Phi}$ represents charge injection from electrode scale. 
\item
cell/pack temperature $T$ (thermal transport): $\frac{\partial T}{dt} = \nabla \cdot (D_T \nabla \Phi) + R_T$, where $R_T$ is heat flux from electrode scale, $D_T$ is heat diffusivity.
\end{itemize}
\MarginPar{JBB:  i am not sure which equations correspond to which scale. can we say we solve 
given systems at particle, given system at electrode and given system as cell}
Uncertainty quantification in this context represents an
extreme scale challenge:
each scale alone is a 3D simulation in a heterogeneous domain;
the difference in scale between the first and last set of equations represents approximately 8 orders of magnitude that must be resolved and coupled in some way;
many such hierarchically coupled runs need to be executed to perform a UQ study.
  
%``Simulation of lithium-ion battery models requires simultaneous evaluation of concentration and potential fields, %in both solid as well as liquid phases. In addition, the porous nature of the battery electrodes leads to highly %nonlinear and heterogeneous electrochemical reaction kinetics." \cite{Subramanian:2009}

Examples of battery models include \cite{Less:2012}, in which a fully resolved 3D electrode is simulated using a combination
of convection-diffusion and Butler-Volmer equations. This model uses approximately 25 parameters derived by the authors from experiment.
Another example is \cite{Garcia2005}, which simulates a 3D electrode, including the constitutive relations that are first steps toward
incorporating stress/strain in the models. This model contains approximately 50 parameters.  Another interesting case is \cite{Kim-etal:2011}, where models at three scales (particle, electrode, cell) are coupled via boundary conditions and forcing terms. This model also contains roughly 50 parameters.

The current paradigm for virtually all battery simulations (including those above)
 is the use as fixed values of effective parameters derived from single-scale experiments.  However, this is problematic: first, we do not propagate uncertainty in 
these parameters through the model; second, we miss the chance to use information \emph{across scales} to optimally understand
the system.  Our proposed work specifically addresses this shortcoming in the current paradigm by offering the means to 
combine modeling and data \emph{across a hierarchy of experiments}
to reduce uncertainty in the underlying physics parameters and improve predictive capability.

The hope for such a program rests on the fact that hierarchies of experiments exist addressing virtually all aspects 
of the heterogeneous battery system.  For example, some representative properties and measurements across scales include:\\
\textbf{diffusivities}: (microscale) single-crystal measurements, molecular diffusivity  \cite{Chung:2011}; (mesoscale) in-situ/single particle measurements \cite{Cui:2012};  (macroscale) galvanostatic intermittent titration technique (GITT) and potentiostatic intermittent titration technique (PITT) (routinely carried out using composite electrodes) \cite{Wen01121979}\\
%Multi-scale measurements hel p resolve disputes in transport mechanism
\textbf{rate constants}: (microscale) reactive transmission electron microscopy (TEM) \cite{Gu2012}; (mesoscale) electrolytic
solution ``design" \cite{Aurbach2004};  
(macroscale) electrochemical impedence spectroscopy (EIS) \cite{Meyers2000} \\
%(only ``effective reactivities" are of interest to the macro-models)  \\
%Multi-scale measurements help eliminate multiple “optima”
\textbf{open circuit potentials, entropy measurements, decomposition potentials}:  
(microscale) composition versus lattice structure (XRD) \cite{Ceder2009,Ohzuku1995};
(mesoscale) electrochemical quartz crystal microbalance (ECQM) \cite{Buttry1992};
(macroscale) C/50 discharge for free energy\\
%“Voltage fade”???, non-stoichiometric “phases”???, hysteresis!!!, decompn. potentials fairly well documented
\textbf{conductivities (electronic, ionic)}:  
(microscale) quantum transport, lithiation mechanisms \cite{Ceder2009};
(mesoscale) properties of individual components combined by porous media theory \cite{Stroud1975};
(macroscale) directly measure effective conductivities \\
%Not strictly multiscale; need to think about this more
\textbf{mechanical properties}: 
(microscale) in-situ TEM \cite{Wang:2012};
(mesoscale) single particle and in situ electrode level strain\cite{Qi:2010,Verbrugge:1999};
(macroscale) pressure transducers 
%Rational design across scales

We propose to use the multiscale implicit sampling and other frameworks developed under this proposal to
address the origin and effects of pressure fluctuations in batteries for electric drive vehicles.  Specifically, we intend to
use macroscopic experiments,
% microscopic calculations (quantum),
micro-(particle level) and meso-scale (electrode level) experiments to
understand the small scale---especially mesoscale (because this is where the
battery designer has the most degrees of freedom)---origin of macroscale
pressure trends known to correlate with battery degradation (e.g.,
capacity fade) and safety (e.g., thermal runaway, a.k.a. fire) issues.
%%%%%%%%%%% (end) batteries

\subsubsection*{Research issues}
The three application areas share a number of commonalities.
Each has a hierarchy of experiments that are used to probe the system.
All involve reaction and diffusion processes.
One characteristic of reaction diffusion processes is that some aspects of the system
may not be observable from the available data. For example, in a reaction chain the slowest
reactions dominate the response so that fast reactions are not effectively probed.
Furthermore, all of the applications
have a potentially complex relation between what is measured and quantity of interest,
even for relatively simple experiments. 
Planar laser induced fluorescence diagnostics in combustion, for example, 
measure photon emissions from excited states that have complex relation to the underlying
composition.
Furthermore, for the more complex experiments, the
quantities of interest are statistical / feature based
particularly at higher levels of the hierarchy.

In spite of the commonalities, the target application areas have a number
of distinct features.
Combustion is more well understood with mathematical models that are on firmer ground
and more sophisticated numerical models.
However, combustion models potentially have more parameters than the other two areas.
Models for photovoltaics are less well established.
For this type of problem 
model inconsistency is a more likely issue in this context.
Aside from model inconsistency, the bounds on the input parameters are typically less well known and much wider than in the combustion context. 
Photovoltaic models also have an inherently stochastic component in the transport at the grain boundaries and the grain growth models 
Batteries are similar to photovoltaics
in that the models are less sophisticated. However battery problems introduce a 
new element, namely that the problem is inherently multiscale.
In this case we need to devise mechanisms to communicate information
between different types of models at different scales.

\subsubsection*{Problem set up and overview of implicit sampling and MCMC}
For each experiment,  we have an operator $L_e$ that depends on the state of the system, $x$, time,
and a set of parameters, $\theta$ that defines the dynamics of the system. For deterministic 
dynamics
\begin{equation}
\label{eq:ModelEquation}
x_t = L_e(x,t,\theta) \qquad \mathrm{with} \qquad x(0) = x_0
\end{equation}
while for stochastic dynamics we have
\[
dx = L_e(x,t,\theta) dt + dW \qquad \mathrm{with} \qquad x(0) = x_0
\]
where $dW$ is a stochastic forcing term. 
Uncertainty of the model is expressed by making the parameters $\theta$ random variables.
For example, $\theta$ can be a Gaussian with mean $\mu$ and covariance matrix $\Sigma_0$.
For each experiment we 
assume we have an approximation for $\mu$ and $\Sigma_0$ that are known from prior investigations.
The initial conditions $x_0$ may be random as well, however we assume that we have a (prior) pdf to describe them.

We are also given data, $y_i$, at various time points $t_i$ in the evolution.
The data is assumed to be a function of $x_i$ and
and we assume that the measurements are perturbed by noise, which, for simplicity, we assume to be Gaussian:
\begin{equation}
	\label{eq:DataEquation}
	y_i = h_e(x_i)+vi,
\end{equation}
where $i=1,2,\dots$, and $v_i$ is are independent Gaussian random variables with mean $0$ and covariance matrix $\Sigma$.
Our goal is to use the data across a range of experiments to reduce the uncertainty in $\theta$ (and possibly the initial conditions $x_0$), and to 
access the degree to which the reduction in uncertainty improves the predictive capability of the model.

Our overall approach is to rely on Bayes' rule to formulate the posterior pdf which we then approximate via Monte Carlo sampling. One generic issue with Monte Carlo approaches
is the need for new algorithms to improve computational efficiency for large-scale problems.
Our target problems also include specific features that must be addressed.
Kinetic models for example often exhibit multiple reaction pathways where
many different parameter sets are nearly as good at explaining the data.
This type of phenomenon, referred to as model degeneracy,
causes isotropic sampling algorithms such as single variable heat bath (Gibbs sampler) or isotropic
Metropolis walk to slow down dramatically.
Another characteristic of our target applications is that
the exact model is not in the family being fit.
Models are only approximations to reality.
For example, the Arrhenius form for reaction rates are only modeling approximations, though they can be very accurate. Similarly, typical models for species diffusion in combustion are only approximations, even at the continuum level,
to a full transport model, which is, in itself, an approximation to the underlying molecular processes.
Even small modeling errors can make an accurate global fit impossible, particularly in chaotic systems.
A more serious structural issue arises in the case of batteries and photovoltaic models where the models
are not yet mature.  In these areas we are likely to encounter structural issues where key physical
processes are missing from the description.
We need statistical methods that are robust to fundamental inconsistencies between models and data.
Finally, we need approaches that can deal with issues that arise from the highly nonlinear observation
functions, $h_e$, characteristic of the target applications.

The central theme of this project will be to develop new smart sampling technologies 
to address these issues.
The types of methods we plan to consider fall into two distinct types:  particle filter approaches
and MCMC.
Both of these approaches aim to reduce the computational work associated with naive MC approaches.
MCMC algorithms define a Markov
process that samples the underlying Gibbs distribution of the problem. 
Evolution of this Markov process
samples the posterior distribution of the system.
When the underlying dynamics is stochastic, MCMC needs to estimate
\MarginPar{JBB:  is this correct? Jonathan?}
the full state of the system in space and time, making it extremely memory intensive.
Particle filters, which arise in the context of data assimilation,
fit into a framework of importance sampling Monte Carlo algorithms.  Here the goal is to use
derivative / sensitivity information to guide the selection of samples to reduce the number of samples
needed to effectively sample the distribution.
%Compared to MCMC, the filtering approach is less memory intensive but more computationally intensive, thus representing
%a tradeoff between memory and computation.
In any given setting, it is not clear which of these approaches will be preferable and
we need to understand the tradeoffs, e.g. between memory and computation, in each method to determine which approach will be most effective for a given experiment. We anticipate that one approach is not optimal across a hierarchy of experiments, rather
that different approaches will be preferred based on specific problem characteristics. Intuitively,
we would expect MCMC to be a more attractive option for relatively simple problems with stochastic dynamics and a particle
filter to be a better choice for deterministic dynamics as problem complexity increases. Quantifying those relationships
and understanding how to transition between approaches are important research questions.
Furthermore, neither of these approaches is new; however, substantial development is needed to meet the requirements
of the applications we are considering.

The starting point for our development of particle filtering methods is the implicit sampling methodology invented at LBNL \cite{chorintupnas,chorin2010,Morzfeld2011,Morzfeld2012,Atkins2013}.
The central idea here is to first search for the regions of high probability with respect to the posterior.
This search can be implemented by numerical optimization.
Once the high probability region is located, samples that lie within this region can be generated by solving simple algebraic equations.
Although the optimization is computationally expensive, implicit sampling was shown to outperform standard MC importance sampling in small geophysical models \cite{Morzfeld2011,Morzfeld2012,Atkins2013}. 
However, for the realistic applications we are considering, substantial development is needed, especially with respect to efficient use of extreme-scale computer architectures. 

To introduce the basic idea,
we consider the case in which the dynamics is deterministic, the initial conditions are known and we only have data at a final time, $T$.
In this case we can combine the model (\ref{eq:ModelEquation}) and data equation (\ref{eq:DataEquation}) to
\begin{equation}
\label{eq:IS_data}
	y_T = H_e(x_0,T,\theta)+v,
\end{equation}
where $H_e$ is the function which is obtained by first running the model up
to time $T$ followed by applying $h_e$ to the state $x_T$. We are interested in the information we can extract from the data about the parameters $\theta$ and
therefore consider the random variable $\theta|y$, which is characterized by its pdf $p(\theta|y)$. Using Bayes' rule, we find that
\begin{equation}
	p(\theta|y_T) \propto p(\theta)p(y_T|x_0,T,\theta),
\end{equation}
where  $p(\theta) = \mathcal{N}(\mu,\Sigma_0)$ is the ``prior'' and $p(y_T|x_0,T,\theta)$ is the ``likelihood'',
which can be read off of (\ref{eq:IS_data}), $p(y_T|x_0,T,\theta)\sim \mathcal{N}(H_e(x_0,\theta),\Sigma)$.
The pdf $p(\theta|y_T)$  is called the ``posterior'' and we wish to approximate it with implicit sampling. 

The general procedure is as follows. Define a function by
\begin{equation}
	F(\theta)= -\log \left(p(\theta)p(y_T|x_0,T,\theta)\right).
\end{equation}
Note that $F$ is a function of the uncertain parameters and that the minimizer of $F$ is the mode of the posterior. Thus, the high probability region of the posterior is the neighborhood of the minimizer of $F$ and we can identify this region via numerical minimization of $F$. To find samples in this region we solve (repeatedly) the algebraic equations
\begin{equation}
\label{eq:IS_sampling_eq}
	F(\theta)-\phi = \frac{1}{2}\xi^T\xi,
\end{equation}
where $\phi = \min F$ and $\xi$ is a Gaussian reference variable with mean zero and unit variance, i.e. the equations have a random RHS.
Note that the RHS is small with a high probability ($\xi$ has mean zero), which implies that the left hand side is also small with a high probability and, therefore, the solution is close to the minimizer of $F$ which is the mode of the posterior.
Thus, the solutions of these equations (for different realizations of $\xi$) are in the neighborhood of the mode of the posterior, i.e. almost all samples are compatible with the data.
Generating samples with low probability with respect to the data (as in standard MC sampling) is avoided by the minimization step. 

There are various methods for solving the algebraic equations of implicit sampling (\ref{eq:IS_sampling_eq}) \cite{chorin2010,Morzfeld2011}.
For example, if information about the curvature of $F$ is available, e.g. from BFGS-type optimization, we can use linear maps based on the Hessian of $F$.
In other situations, e.g. derivative free optimization or optimization using surrogates, this information may not be available.
In this situation we can use the random map approach, where one chooses a direction at random, and then computes a solution of (\ref{eq:IS_sampling_eq}) in this direction.
With this random map approach, only a single equation in a single variable needs to be solved (regardless of the number of parameters being estimated). We plan to research into optimal choices for solving the algebraic equations for each target application. In summary, implicit sampling amounts to the following two steps:
\begin{enumerate}
	\item Minimize the function $F$ to identify relevant regions of the parameter space
	\item Find samples in this region by solving the random algebraic equations (\ref{eq:IS_sampling_eq})
\end{enumerate}
\MarginPar{JBB:  Matti, it seems like we should say more abou t solving the random equations}
While the general method of attack is clear, many research questions arise when applying this sampling technique to the target applications (see below for detail).

\MarginPar{Jonathan please fix}
The other basic technique we plan to use is MCMC.
MCMC is basically a two step process.
In a deterministic parameter estimate mode, given the parameters $\theta_\tau$ at step $\tau$,
we first generate a proposal $\theta^*$ for new values of the parameters and compute
the probability of the posterior with respect to $\theta^*$ compared to $\theta_\tau$.
In the present setting this requires running a forward simulation.  We then apply an
acceptance criterion to either accept the new proposal and setY$\theta_{\tau+1} = \theta^*$
or reject it and let set $\theta_{\tau+1} = \theta_\tau$.
When the dynamics is stochastic we include an estimation of the state, $X$,
in the process where here $X$ represent state in space and time.
Although for high dimensional problems MCMC approaches are considerably more cost
effective than naive sampling approaches based on drawing independent propsals
from the prior, they can
exhibit a critical slowing down phenomenon for poorly conditioned problems.
The length of time that
one needs to simulate the Markov process to obtain good results depends on the autocorrelation time
of the underlying process, which can be extremely long for high dimensional systems.
Several approaches have been proposed to deal with this problem.
\MarginPar{JBB:  Jonathan check}
Here we will focus on multigrid MCMC \cite{Goodman1989}
and parallel marginalization \cite{Weare2007}.
Multigrid MCMC is similar in spirit to multigrid for a linear system.  One
defines a sequence of coarsened versions of the problem along with an effective
dynamics at the coarsened scale.  One then performs what is essentially a standard
multigrid W-cycle except that instead of relaxation, one performs a simple sampling
approach such as a Gibbs sampler.  Under certain conditions, one can rigorously
prove that multigrid MCMC completely eliminates critical slowing down as the
system size increases.
Parallel marginalization is similar in spirit. It evolves a sequence of coarsened
version of the Markov process
simultaneously with the fine scale version of the process. 
The distributions for the coarsened processes are obtain as marginals of the original
distribution.  In addition to normal proposals within each process, we also consider
swap proposals between chains of adjacent resolution.  Proper choice of the acceptance
criteria for swaps guarantees that the overall combination of the processes converges
to the correct distribution and dramatically reduces correlation time.

\subsubsection*{UQ across a hierarchy of experiments}
A characteristic of all our target applications is that there is a hierarchy of experiments.
How to use this characteristic for successful and efficient sampling is a major research question we will address. In the data assimilation context in which particle filters originated, the algorithms move through a temporal sequence of data and sequentially update the state/parameters as data becomes available.
The update rule comes from repeated applications of Bayes' rule and ultimately leads to a recursive formulation of the posterior.
Here we need to modify the methodology to transition from one experiment to the next as we move through the hierarchy of experiments, i.e. we need to find the update rule for moving up in the hierarchy.
For example, we can construct priors at a higher level of the hierarchy from posteriors at lower levels. For implicit sampling this amounts to finding a representation of the posterior at a lower level that can serve as a prior in the optimization at the next stage.
Moreover, the theory and numerics can be intertwined here because the models obtained (as posteriors) at lower levels of the hierarchy may help with speeding up the minimizations required at higher levels of the hierarchy (see below for more detail). 
Another central issue is determining whether or not moving through the hierarchy of experiments from the simplest to the most complex introduces bias in the parameter estimates. If this bias is found to be significant, strategies must be developed to reduce that bias. 

\subsubsection*{Model degeneracy and model inconstancies}
Another key problem that has to be addressed is model degeneracy.
A model has an approximate degeneracy if many different parameter sets are nearly as good at explaining the data.
For example, if there are multiple reaction pathways in a kinetic description,
certain combinations of reaction rates may be much better
estimated than the individual rates. Model degeneracies can lead to difficulties with implicit sampling, since these corresponds to ``valleys'' in  the function $F$.
The valleys slow down the convergence of the required optimizations and, more importantly, a good approximation of the degeneracy requires the valleys being populated with samples which will increase the number of samples one needs to generate.
The computational expense of implicit sampling will thus increase.
To fix these issues, we will investigate how to connect MCMC to implicit sampling to speed up exploration of model degeneracies.
However, current MCMC algorithms, e.g. isotropic sampling algorithms such as single variable heat bath (Gibbs sampler) or isotropic Metropolis walk will also be slow when model degeneracies occurs.
We plan to use the affine sampling approaches developed at NYU to address issues of model degeneracy \cite{GoodmanWeare2010}.
The idea here is to use an ensemble of $N$ samplers instead of just one. 
If $N$ is sufficiently large, an ensemble sampler can be constructed that is affine invariant; i.e., it leaves the ensemble
random walk unchanged under affine transformation.  This property guarantees that the method will work well for problems that can
be rescaled to be well condition with an affine transformation.
\MarginPar{(matti): Jonathan, does this make sense?}

It is inevitable that the exact model is not in the family being fit (even at the highest level of the hierarchy) and
the statistical methods we develop must be robust to this situation.
For example, the Arrhenius form for reaction rates are only modeling approximations,
though they can be very accurate. Similarly,
typical models for species diffusion in combustion are only approximations, even at the continuum level,
to a full transport model, which is, in itself, an approximation to the underlying molecular processes.
Even small modeling errors can make an accurate global fit impossible, particularly in chaotic systems.
A more serious structural issue arises in the case of batteries and photovoltaic models where the models
are not yet mature.  In these areas we are likely to encounter structural models where key physical
processes are missing from the description.
One approach is to include noise in the dynamics, so that the posterior distribution does not require the
dynamical equations to be satisfied exactly.
This increases the complexity of both MCMC and implicit sampling approaches. In the MCMC case, the Markov chain
must describe the entire space time trajectory of the solution. Somewhat analogously, an implicit sampling approach must
estimate the entire state.
An interesting intermediate position is to only introduce stochastic perturbations periodically.  The system
can be deterministic evolved between perturbations. This allows
us to have some tradeoff between computation and memory.
Balancing that tradeoff with architectural features and fidelity of the algorithms needs to be explored.
We will conduct computational experiments to study this problem, then use the results to 
choose appropriate noise levels for our physical models.
By combining what is known about the error levels in both model and data, we can also estimate upper and lower bounds
of the predictive skills of the resulting stochastic models.
\MarginPar{can be develop criteria to assess if something is missing form model}

\subsubsection*{Efficient sampling in view of complex observation operators}
%Another feature of all three target applications is a complex relation between the data and the parameters being estimated. For example, laser diagnostic measurements depend on the system state but there are parameters describing the laser interaction (so-called quenching coefficients) that have uncertainties as well, i.e. the observation function $h$ itself can be uncertain. We plan on extending the implicit formalism to estimate uncertainties in the measurement mechanisms. 
Another feature of all three target applications is a complex relation between the data and the parameters being estimated.
\MarginPar{JBB:  made this more specific . . .perhaps too much. . .left original text}
For example, in
laser diagnostic measurements of a species S,
the intensity, $I_{{LIF}}$,
for weak, non-perturbing laser excitation is given by
\begin{equation}
I_{{LIF}} \; = \; c_{cal} \; I_{{laser}} \; N_{\rm S} \sum_i 
f_{B,i}
(T) \; B_{i,k} \; g_{\lambda,i} (p, T, X) \sum_{k,j} {A_{k,j} \over \sum_\ell A_{k,\ell} + Q_k (p
,
T, X)} \; .
\label{eq:quench}
\end{equation}
Thus the measured signal
depends on the number density of
the excitable molecules (number density $N_{\rm S}$
times the Boltzmann fraction $f_{B,i}$), the Einstein coefficient
$B_{i,k}$ for
absorption $i \rightarrow k$, the spectral overlap fraction
$g_{\lambda,i} (p, T, X)$ of the laser profile, the
absorption spectrum of species S, and the fluorescence quantum yield
$A/(\sum A + Q)$, where $A$ and $Q$ are decay rates due to
spontaneous emission and electronic quenching, respectively.
Although
equation \ref{eq:quench} provides a quantitative relationship
between the measured density of S and laser intensity, 
the uncertainties in the various terms introduce additional sources of uncertainty
into the measure function $h$.

We plan on extending the implicit formalism to estimate uncertainties in the measurement mechanisms.
In addition, the actual measure quantity is
an extremely complex function of the state.
This type of complex relation can impact the choice of mapping from $\xi$ to $x$ in the implicit sampling algorithm,
We can investigate
this issue by using synthetic experiments that transition from viewing the data
as a simple projection of the state through increasing levels of complexity up to
a computational model of the actual measurement.  From this type of experiment we can 
analyze how different aspects of the measurement alter the information we can
obtain from the measurement. This type of analysis will also allow us to assess the
role of noise in highly nonlinear observations of the state.

Moreover, at high levels of the hierarchy in turbulent combustion, we are interested in finding parameters that generate features one observes in the data which are insensitive to details of the state trajectory. In fact, we want to avoid trying to estimate a trajectory that is a point-wise fit to the trajectory given by a measurement because it is a hopeless task.
\MarginPar{JBB:  is this too hopeless? (matti) I hope not, I would like to try.}
On an abstract level, this corresponds to the problem of translating qualitative behavior observed in the data into quantitative information about parameters (by using sampling techniques) and, to the best of our knowledge, this fundamental problem has not been addressed before. We anticipate that tackling this problem will require a careful re-evaluation and perhaps re-definition of the observation function $h$ and we plan to take first steps towards a methodological and quantitative assimilation of data features. 
A major hurdle here is that we cannot use adjoints for the optimization needed by the implicit sampling approach.  Instead we will need
some type of surrogate representation of the likelihood to perform the optimization.

\subsubsection*{Multi scale sampling}
Another central research question is how to use sampling for multi-scale problems.
\MarginPar{at this point this is sampling only ... how would MCMC work. . Jonathan? I tried to disconnect the first part of the discussion from IS. This way we can talk about multi scale IS and MCMC}
Specifically, we will address how information can be propagated efficiently across different scales.
For example, in battery simulation, we are interested in the fluctuation of pressure with time since it can critically affect performance, degradation, and safety of batteries.
This ``macro scale'' (here: millimeters) pressure is the manifestation of many effects at the ``mesoscale'' (here: microns): conversion of electrolyte to the gas phase; the sponge like nature of the porous electrode; elastic properties of the binder (the binder is the ``glue'' that holds the particles in their porous structure); the porosity and toruosity of the separater.
The pressure is the sum of these effects combined across the whole cell and can be measured via detailed signals from transducers. In addition, we have data of mesoscale phenomena such as stress, strain, swelling, and other mechanical properties.
Our goal is to incorporate all available data to form an estimate of the pressure and the mesoscale state, described in terms of layer thicknesses (substrate, epoxy layer, current collector, active material, entire electrode), volume fractions (particles, binder, pores), porosity, tortuosity, particle sizes and distributions (see, e.g., \cite{Sethuraman2012334}) and other parameters governing the makeup and morphology of the heterogeneous
electrode.
By using Bayes' rule we can factorize the conditional pdf of the cell-level pressure and electrode-level stress and strain, given both
cell and electrode level data:
\begin{equation}
\label{eq:MultiScalePDF}
	p(x^1, x^2|y^1,y^2) \propto p(y^1|x^1) p(y^2|x^2) p(x^2|x^1)p(x^1),
\end{equation}
where we use the indices to distinguish scales, i.e. $x^1,y^1$ correspond to the mesoscale state and data, $x^2,y^2$ to the macro scale state and data.
Here, the first two terms come from a model of how the data is connected to the state (at each scale); the third term describes how the various mesocsale quantities affect pressure and the last term is the prior for the mesoscale parameters.
The above equation demonstrates how both the data and physical models are being incorporated and combined across scales.
By switching the indices, we can use this formalism to push information from macro- to mesoscales, which may be important since macro-scale measurements are typically more reliable than meso-scale measurements. In this case, the physical models ($p(x^2|x^1)$) acts as constraint (what $x^1$ is compatible with $x^2$?) rather than as a prediction (what is $x^2$, given $x^1$?) because of the many-to-one relation between mesoscale parameters and macroscopic pressure. A research question it to assess the validity of the assumptions that lead to a factorization of the pdf, in particular we need to verify that the scale coupling models are (nearly) ``Markovian'', i.e. they couple only two subsequent scales, and that the observation models at each scale only depend on the state at that scale. 

A successful and efficient solution of this multi scale problem allows physics-based models (e.g. how pressure arises from mesoscale phenomena) to inform our interpretation of macroscale measurements, or, conversely, the physics-based model can ``learn" from the macro-scale measurements. We plan to develop a multi-scale version of implicit sampling to approximate the above (multi scale) pdf (\ref{eq:MultiScalePDF}) and, in particular, we will address implementation issues, e.g. how will the coupling between scales affect the required optimizations and subsequent solves of~(\ref{eq:IS_sampling_eq}). We expect that the required computations will be extreme scale, since the problem requires resolving and coupling of phenomena that vary over 10 orders of magnitude.

\subsubsection*{Implementation, numerics and extreme scale computation}
There are also many implementation issues that to need be addressed, especially with respect to efficient scaling of the MC algorithms on massively parallel computers.
In implicit sampling the minimization is the computational bottleneck and its efficient implementation is crucial for the success of the method.
Moreover, the minimization algorithm will likely depend on where we are in the hierarchy of experiments.
One of our specific research topics will be to address efficient minimization at each level of the hierarchy, especially in view of massively parallel computer architectures.
For example, we can consider coupling adjoint codes for gradient computations of $F$ to BFGS-type algorithms using a parallel optimizer such as TAO from Argonne.
At high levels of the hierarchy, adjoint codes may be out of reach or too costly and time consuming to construct.
In these cases, derivative free optimization methods must be considered.
Another possibility is to use simplified models for the minimization.
For example, we can borrow ideas from multi-grid and run the minimizations on a coarse grid, while doing e.g. forecasting on the fine grid.
Alternatively, we can use a surrogate method, where a small number of forward simulations are used to generate a simplified model of the simulation.
This model is then used in the optimization and its further refinement goes hand in hand with its use in seeking the minimum.
Many research questions surround the interplay between optimization and sampling, particularly with approximate surrogate models.
For example, we can use numerical experiments to find a characterization of how errors introduced by a simplified model impact the overall behavior of the algorithm and what must be known about the errors of the simplified model to obtain such a characterization.  
\MarginPar{George: The optimization algorithm is sequential.  It is going to be a bottleneck in a sampling algorithms where everything else can be embarrassingly parallelized on an exascale machine.  }

We will reduce the complexity of the response that reduced order models are required to emulate by modeling the difference between the simple and the complex models. A promising approach that is consistent with~(\ref{eq:IS_data}) is GPR. 
Consider outputs from two different models, $H^S$ and $H^C$ where $H^C$ is deemed more accurate than $H^S$. 
Then we can write (\ref{eq:IS_data}) as 
\begin{equation}
y(\theta) = H^S(\theta) + (H^C(\theta) - H^S(\theta)) + v,
\end{equation}
and GPR can be used to model  $(H^C - H^S)$ as $\mathcal{N}(m_{\rm GPR}(\theta;\bar{\theta}),\Sigma_{\rm GPR}(\theta;\bar{\theta}))$, where $\bar{\theta}$ are sample points used to construct the GPR model. Since both $m_{\rm GPR}$ and $\Sigma_{\rm GPR}$ depend on $\theta$, this approach is capable of modeling the nonlinearity in the difference. Application of implicit sampling (see above) to this  formulation seems feasible, however appropriate models for $m_{\rm GPR}$ and $\Sigma_{\rm GPR}$ and their construction procedures that efficiently utilize extreme scale computers are research questions that need to be answered. 

%This type of reduced-order models can also play a role in improving the efficiency of MCMC algorithms.  Several of the techniques for improving MCMC involve
%\MarginPar{JBB:  added this . . too tired to decide if it is just bs.  Jonathan?}
%evolving coarser changes with algorithms for sharing information between those chains.  Typically the relationship between coarse and fine chains
%correspond to simple averaging and sampling procedures.  However, we can potentially consider a much wider range of ``coarse'' models where a reduced-order model
%is used to map between ``coarse'' and fine levels.
%
%\MarginPar{JG doubts this will work and notes a need to for derivative information to guide sampling. JBB
%doubts we can perform adjoint simulations for 3 turbulent simulations.  Only way out JB can think of
%is based on discussion with George about combining coarse simulation with a statistical surrogate to
%build an estimate of finer response . . . Unless someone has a brilliant idea here i suggest we leave this
%for the preproposal and thing of how to address it in real proposal}
%\MarginPar{need to decide if there is something viable here. Another alternative that merits consideration
%is using the adjoint of a simpler model for the optimization.}

Several other practical issues with will be addressed with both implicit sampling and MCMC.
For any given experiment, we may have access to data at more than one time and we may have than one type of data.
There are two options for using these data for estimation:
(\emph{i}) we can extend the
%implicit
sampling formalism to include all $m$ data sets and estimate the parameters using all the data (off-line estimation); or
(\emph{ii}) we can estimate the parameters using a batch of $k_1$ data sets and then refine this estimate using
the remaining data in batches of $k_n$ sets, i.e. we can move through the data sequentially (on-line estimation).
Theoretically, off-line estimation seems more attractive, since more data should lead to more accurate estimates because it avoids bias (e.g. the maximum likelihood estimator is asymptotically unbiased as the number of data goes to infinity). However in practice one often finds an ``optimal'' number of data sets per estimation sweep (i.e. an optimal $k$), the reason being (at least in part) that the exact model is not in the family being fit (even at high levels of  accuracy).
This is well known and cleverly used in numerical weather prediction and we plan to investigate the optimal number of data sets per estimation sweep for our target applications. 

In implicit sampling, we may need to relax the Gaussian assumptions for the prior and likelihood. For example, a Gaussian prior is not meaningful if the parameter is known to be positive. Similarly, we have chosen a Gaussian ``reference variable'' $\xi$, but other choices are also possible. We plan to investigate the interplay between priors, likelihood functions and the reference variable in implicit sampling and will determine good choices for reference variables for each target application and at each level of the hierarchy.

\MarginPar{need to feed some specifics back into text or put specific targets with apps into timetable}
To evaluate the methodology we will examine test cases within each of our target application areas.  In particular,
we will consider thermodiffusive instabilities in turbulent hydrogen flames, robustness of Li-ion batteries under abusive conditions
and the formation of point defects in new photovoltaic materials.  In each case, we will focus on how uncertainties in the 
model parameters influence predictive capability and how we can reduce uncertainty using a hierarchy of experimental data.


\subsection*{Timetable of Activities}

RESERVED 1/3 - 1/2 page for this

\subsection*{Project Objectives}

The goal of this project is to develop a mathematical framework  
based on novel sampling methods that
intertwines parameter estimation and simulation 
to estimate uncertainty and improve prediction for target systems.
Specifically, we want to
(1) use available data from a hierarchy
of experiments of increasing scale and complexity to restrict
uncertainty in the description of the system, (2) estimate the impact of the improved characterization
on predictive capability and (3) identify which of the remaining uncertainties have the most impact
on the uncertainty of predictions.
We will demonstrate the use of the framework for prototype problems in combustion,
novel photovoltaic material and lithium-ion batteries.


\bibliographystyle{plain}

\bibliography{george_rom,pd,batteries3} 

\end{document}
